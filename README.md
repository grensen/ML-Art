# ML-Art
Some stuff around my ML work

---

## The Neural Network

A dropout neural network with the working neurons
<p align="center">
  <img src="https://github.com/grensen/ML-Art/blob/master/dropout_art.png?raw=true">
</p>

---

## Spiral Multiclass Dataset

<p align="center">
  <img src="https://github.com/grensen/ML-Art/blob/master/figures/spiral_demo.png?raw=true">
</p>

---

## Custom Perceptron

<p align="center">
  <img src="https://github.com/grensen/ML-Art/blob/master/custom_perceptron.png?raw=true">
</p>

---

## Mixed Linearity Network

A deep neural network + multiclass logistic regression = mixed linearity network.

<p align="center">
  <img src="https://raw.githubusercontent.com/grensen/ML-Art/master/figures/mixed_linearity_network.png?raw=true">
</p>

## Custom Autoencoder

A custom network can also be an autoencoder

<p align="center">
  <img src="https://raw.githubusercontent.com/grensen/ML-Art/master/figures/custom_autoencoder.png?raw=true">
</p>

---


## The Math behind

The deep relationship between machine learning and e = (1+(1/N))^N. 
If I try to think about e, I imagine it as a spectrum of grey colors.

<p align="center">
  <img width="" height="" src="https://github.com/grensen/ML-Art/blob/master/euler_tricks.png">
</p>
A full explanation of why ln() and exp() are so important would fill entire books. But with a basic understanding it's clear to me how wonderful this visual expression is.

---

## Visualising Data

60.000 dots in 10 colors, a trivial message behind the graph or my first try to visualize the entire MNIST dataset.
For the effects the [Intel fast_rand()](https://software.intel.com/en-us/articles/fast-random-number-generator-on-the-intel-pentiumr-4-processor)
function comes in.




<p align="center">
  <img width="" height="" src="https://github.com/grensen/ML-Art/blob/master/60000_dots_and_10_colors_jiw.jpg">
</p>
The first reaction from my girl friend was "this looks like tv noise in colored", and I was happy. :-)

---


## Reinforcement Learning 

A maze with Q-Learning, inspired by [Alexander Amini's](https://www.youtube.com/watch?v=nZfaHIxDD5w) great lecture on RL, my challenge started.


<p align="center">
  <img src="https://github.com/grensen/ML-Art/blob/master/maze_unsolved.png">
</p>

Credits for this idea goes to [Dr. James D. McCaffrey](https://docs.microsoft.com/en-us/archive/msdn-magazine/2018/august/test-run-introduction-to-q-learning-using-csharp)

The algorithm here based on his code.

---

Did you solved the maze? My agent did. :-)


<p align="center">
  <img src="https://github.com/grensen/ML-Art/blob/master/maze_solved.png">
</p>

Really good fun!

---

## Datasets

A nice visualization of a dataset.

<p align="center">
  <img width="" height="" src="https://github.com/grensen/ML-Art/blob/master/dataset.png">
</p>

Credits for this idea goes to professor [JÃ¶rn Loviscach](https://www.youtube.com/watch?v=41SdVA2aqKw/)

---

## The Parity Patterns

<p align="center">
  <img width="" height="" src="https://raw.githubusercontent.com/grensen/ML-Art/master/parity_00.png">
</p>

The parity patterns I've meet early 2019, quite interesting

<p align="center">
  <img width="" height="" src="https://raw.githubusercontent.com/grensen/ML-Art/master/parity_01.png">
</p>

A google search after that art of patterns was not succesful

<p align="center">
  <img width="" height="" src="https://raw.githubusercontent.com/grensen/ML-Art/master/parity_02.png">
</p>

A dropout pattern seems like a way to use these patterns

<p align="center">
  <img width="" height="" src="https://raw.githubusercontent.com/grensen/ML-Art/master/parity_03.png">
</p>
